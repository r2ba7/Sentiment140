{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports import *\n",
    "from utils import *\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaModel\n",
    "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\n",
    "import pytorch_lightning as pl\n",
    "from transformers import RobertaTokenizerFast\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import transformers\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    PATH = 'training.1600000.processed.noemoticon.csv'\n",
    "    LR = 1e-5\n",
    "    MAX_LEN = 64\n",
    "    BATCH_SIZE = 64\n",
    "    SEED = 42\n",
    "    train_ratio = 0.9\n",
    "    test_ratio = 0.1\n",
    "    num_workers = 8\n",
    "    roberta_model = \"roberta-base\"\n",
    "    tokenizer = RobertaTokenizerFast.from_pretrained(roberta_model)\n",
    "\n",
    "def log_execution(func):\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        logging.info(f\"Executing {func.__name__}\")\n",
    "        result = func(*args, **kwargs)\n",
    "        logging.info(f\"Finished executing {func.__name__}\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "def timing_decorator(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        print(f\"Function {func.__name__} took {end_time - start_time} seconds to run.\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "def seed_everything(seed=Config.SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentiment140Dataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.path = Config.PATH\n",
    "        self.tokenizer = Config.tokenizer\n",
    "        self.cleaned = False\n",
    "        self.load_data()\n",
    "        \n",
    "    @log_execution    \n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Loads the data.\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(self.path, header=None, names=['targets', 'ids', 'date', 'flag', 'user', 'text'], \n",
    "                           encoding='latin-1')\n",
    "        self.data.targets = self.data.targets.replace({4: 1})\n",
    "        self.check_for_dups()\n",
    "        # self.check_targets()\n",
    "        # self.X, self.y = self.data.text, self.data.targets #Series\n",
    "        # self.X, self.y = self.data.text.to_numpy(), self.data.targets.to_numpy().astype(np.uint8) #numpy\n",
    "        self.X, self.y = self.data.text.tolist(), self.data.targets.tolist() #List\n",
    "\n",
    "    @timing_decorator\n",
    "    def deep_clean(self):\n",
    "        # List: 370.0727105140686 seconds to run.\n",
    "        # Series: 372.0254681110382 seconds to run.\n",
    "        # Numpy: 371.67559838294983 seconds to run.\n",
    "        # For list\n",
    "        # Add stop words removal\n",
    "        self.X =  list(map(TextPreprocessor.preprocess_text, self.X))\n",
    "        # self.X =  list(map(TextPreprocessor.clean_text, self.X))\n",
    "        # self.X =  list(map(TextPreprocessor.remove_stopwords, self.X))\n",
    "        # self.X =  list(map(TextPreprocessor.stemming, self.X))\n",
    "        # For Numpy\n",
    "        # self.X = np.vectorize(TextPreprocessor.decontract)(self.X)\n",
    "        # self.X = np.vectorize(TextPreprocessor.clean_text)(self.X)\n",
    "        # self.X = np.vectorize(TextPreprocessor.stemming)(self.X)\n",
    "        # # For Series\n",
    "        # self.X = self.X.apply(TextPreprocessor.decontract)\n",
    "        # self.X = self.X.apply(TextPreprocessor.clean_text)\n",
    "        # self.X = self.X.apply(TextPreprocessor.stemming)\n",
    "\n",
    "    def apply_cleaning(self):\n",
    "        if not self.cleaned:  # check if data has been cleaned\n",
    "            self.deep_clean()\n",
    "            print(\"Done cleaning data\")\n",
    "            self.cleaned = True\n",
    "    \n",
    "    def find_max_len(self):\n",
    "        self.max_len = self.data['text'].str.len().max()\n",
    "        print(\"Maximum Length: \",self.max_len)\n",
    "        \n",
    "    def check_targets(self):\n",
    "        print(\"Target value counts:\", self.data.targets.value_counts())\n",
    "\n",
    "    def check_for_dups(self):\n",
    "        # print('number of duplicates: ', self.data.text.duplicated().sum())\n",
    "        if self.data.text.duplicated().sum() > 0:\n",
    "            self.data.drop_duplicates('text', inplace=True)\n",
    "            # print(\"Done removing duplicates\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        self.apply_cleaning()\n",
    "        X, y = self.X[i], self.y[i]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            X,\n",
    "            add_special_tokens = True,\n",
    "            max_length=Config.MAX_LEN,\n",
    "            pad_to_max_length=True,\n",
    "            truncation='longest_first',\n",
    "            # truncation=True,\n",
    "            # padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        input_ids = encoding[\"input_ids\"][0] #[0]\n",
    "        attention_mask = encoding[\"attention_mask\"][0] #[0]\n",
    "        labels =  torch.tensor(y, dtype=torch.float)\n",
    "        return {'text': X,\n",
    "                'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask,\n",
    "                'labels': labels\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Executing load_data\n",
      "INFO:root:Finished executing load_data\n"
     ]
    }
   ],
   "source": [
    "def prepare_loaders():\n",
    "    ds = Sentiment140Dataset()\n",
    "    seed_everything()\n",
    "    dataset_size = len(ds)\n",
    "    indices = list(range(dataset_size))\n",
    "    split = int(np.floor(Config.test_ratio * dataset_size))\n",
    "    seed_everything()\n",
    "    np.random.shuffle(indices)\n",
    "    train_indices, test_indices = indices[split:], indices[:split]\n",
    "\n",
    "        # create samplers for train and test sets\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "        # create data loaders for train and test sets\n",
    "    train_loader = DataLoader(ds, batch_size=Config.BATCH_SIZE, sampler=train_sampler)\n",
    "    val_loader = DataLoader(ds, batch_size=Config.BATCH_SIZE, sampler=test_sampler)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "train_loader, val_loader = prepare_loaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoBERTaModel(pl.LightningModule):\n",
    "    def __init__(self)-> None:\n",
    "        super().__init__()\n",
    "        self.prepare_loaders()\n",
    "        self.roberta = RobertaModel.from_pretrained(Config.roberta_model)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.classifier = nn.Linear(self.roberta.config.hidden_size, 1)\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def prepare_loaders(self):\n",
    "        ds = Sentiment140Dataset()\n",
    "        seed_everything()\n",
    "        dataset_size = len(ds)\n",
    "        indices = list(range(dataset_size))\n",
    "        split = int(np.floor(Config.test_ratio * dataset_size))\n",
    "        seed_everything()\n",
    "        np.random.shuffle(indices)\n",
    "        train_indices, test_indices = indices[split:], indices[:split]\n",
    "\n",
    "        # create samplers for train and test sets\n",
    "        train_sampler = SubsetRandomSampler(train_indices)\n",
    "        test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "        # create data loaders for train and test sets\n",
    "        self.train_loader = DataLoader(ds, batch_size=Config.BATCH_SIZE, sampler=train_sampler)\n",
    "        self.val_loader = DataLoader(ds, batch_size=Config.BATCH_SIZE, sampler=test_sampler)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        # self.prepare_data()\n",
    "        return self.val_loader\n",
    "\n",
    "    def forward(self, input_ids, attention_mask)-> torch.Tensor:\n",
    "        output = self.roberta(input_ids=input_ids,\n",
    "                              attention_mask=attention_mask)\n",
    "        pooled_output = output.pooler_output\n",
    "        # dropout_output = self.dropout(pooled_output)\n",
    "        return self.classifier(pooled_output)\n",
    "    \n",
    "    def accuracy(self, preds, labels):\n",
    "        \"\"\"\n",
    "        Computes accuracy for binary classification task.\n",
    "        \"\"\"\n",
    "        # round predictions to the closest integer\n",
    "        rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "        # compute accuracy\n",
    "        acc = (rounded_preds == labels).float().mean()\n",
    "        return acc\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, labels = batch['input_ids'], batch['attention_mask'], batch['labels']\n",
    "        outputs = self(input_ids, attention_mask)\n",
    "        loss = self.loss_fn(outputs.view(-1), labels.view(-1))\n",
    "        acc = self.accuracy(outputs.view(-1), labels.view(-1))\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        self.log('train_acc', acc, prog_bar=True)\n",
    "        return {\"loss\": loss, \n",
    "                \"acc\": acc}\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, labels = batch['input_ids'], batch['attention_mask'], batch['labels']\n",
    "        outputs = self(input_ids, attention_mask)\n",
    "        loss = self.loss_fn(outputs.view(-1), labels.view(-1))\n",
    "        acc = self.accuracy(outputs.view(-1), labels.view(-1))\n",
    "        self.log(\"valid_loss\", loss)\n",
    "        self.log('valid_acc', acc, prog_bar=True)\n",
    "        return {\"loss\": loss, \n",
    "                \"acc\": acc}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        # return optimizer\n",
    "    \n",
    "        param_optimizer = list(self.named_parameters())\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_parameters = [\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\": 0.001,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        return transformers.AdamW(optimizer_parameters, lr=Config.LR)\n",
    "\n",
    "    def predict(self, text):\n",
    "        encoded_text = Config.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens = True,\n",
    "            max_length=Config.MAX_LEN,\n",
    "            pad_to_max_length=True,\n",
    "            truncation='longest_first',\n",
    "            # truncation=True,\n",
    "            # padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        output = self(encoded_text['input_ids'][0], encoded_text['attention_mask'][0])\n",
    "        probabilities = torch.softmax(output.logits, dim=1)\n",
    "        predicted_label = torch.argmax(probabilities, dim=1)\n",
    "        return predicted_label.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Executing load_data\n",
      "INFO:root:Finished executing load_data\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type              | Params\n",
      "-------------------------------------------------\n",
      "0 | roberta    | RobertaModel      | 124 M \n",
      "1 | dropout    | Dropout           | 0     \n",
      "2 | classifier | Linear            | 769   \n",
      "3 | loss_fn    | BCEWithLogitsLoss | 0     \n",
      "-------------------------------------------------\n",
      "124 M     Trainable params\n",
      "0         Non-trainable params\n",
      "124 M     Total params\n",
      "498.586   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "103bb124db5847f1832f69af747962d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function deep_clean took 582.6112864017487 seconds to run.\n",
      "Done cleaning data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62108affdda8447dbf38de719761f582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f3829d2426a47788d630ae156d3c836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "INFO:root:Executing load_data\n",
      "INFO:root:Finished executing load_data\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 39\u001b[0m\n\u001b[0;32m     37\u001b[0m best_model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mload_from_checkpoint(best_model_path)\n\u001b[0;32m     38\u001b[0m torch\u001b[39m.\u001b[39msave(best_model\u001b[39m.\u001b[39mstate_dict(), \u001b[39m'\u001b[39m\u001b[39mbest_model.pt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 39\u001b[0m model\u001b[39m.\u001b[39;49mpredict(\u001b[39m\"\u001b[39;49m\u001b[39mI hate you\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[7], line 105\u001b[0m, in \u001b[0;36mRoBERTaModel.predict\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, text):\n\u001b[0;32m     95\u001b[0m     encoded_text \u001b[39m=\u001b[39m Config\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mencode_plus(\n\u001b[0;32m     96\u001b[0m         text,\n\u001b[0;32m     97\u001b[0m         add_special_tokens \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m         return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    104\u001b[0m     )\n\u001b[1;32m--> 105\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(encoded_text[\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m0\u001b[39;49m], encoded_text[\u001b[39m'\u001b[39;49m\u001b[39mattention_mask\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m0\u001b[39;49m])\n\u001b[0;32m    106\u001b[0m     probabilities \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msoftmax(output\u001b[39m.\u001b[39mlogits, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    107\u001b[0m     predicted_label \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(probabilities, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 36\u001b[0m, in \u001b[0;36mRoBERTaModel.forward\u001b[1;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_ids, attention_mask)\u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m---> 36\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroberta(input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[0;32m     37\u001b[0m                           attention_mask\u001b[39m=\u001b[39;49mattention_mask)\n\u001b[0;32m     38\u001b[0m     pooled_output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mpooler_output\n\u001b[0;32m     39\u001b[0m     \u001b[39m# dropout_output = self.dropout(pooled_output)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\abdel\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\abdel\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:805\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    802\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    803\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou have to specify either input_ids or inputs_embeds\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 805\u001b[0m batch_size, seq_length \u001b[39m=\u001b[39m input_shape\n\u001b[0;32m    806\u001b[0m device \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39mdevice \u001b[39mif\u001b[39;00m input_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m inputs_embeds\u001b[39m.\u001b[39mdevice\n\u001b[0;32m    808\u001b[0m \u001b[39m# past_key_values_length\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "   monitor='val_loss',\n",
    "   min_delta=0.00,\n",
    "   patience=2,\n",
    "   verbose=False,\n",
    "   mode='min'\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='valid_loss',\n",
    "    dirpath='checkpoints',\n",
    "    filename='model-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=3,\n",
    "    mode='min',\n",
    ")\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "# training_args = pl.TrainingArguments(\n",
    "#     ,\n",
    "#     output_dir='results_roberta',          # output directory\n",
    "#     overwrite_output_dir = True,\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     save_strategy=\"epoch\",\n",
    "#     load_best_model_at_end=True\n",
    "# )\n",
    "\n",
    "model = RoBERTaModel()\n",
    "trainer = pl.Trainer(accelerator='gpu',\n",
    "                     max_epochs = 1,\n",
    "                     callbacks=[checkpoint_callback]\n",
    ")\n",
    "trainer.fit(model)\n",
    "best_model_path = checkpoint_callback.best_model_path\n",
    "best_model = model.load_from_checkpoint(best_model_path)\n",
    "torch.save(best_model.state_dict(), 'best_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Executing load_data\n",
      "INFO:root:Finished executing load_data\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "pred_model = model.load_from_checkpoint('checkpoints/model-epoch=00-val_loss=0.00.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_sentiment(pred):\n",
    "    if pred == 0:\n",
    "        return \"Negative\"\n",
    "    elif pred == 1:\n",
    "        return \"Positive\"\n",
    "    else:\n",
    "        return \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = Config.tokenizer.encode_plus(\n",
    "        \"I hate you\",\n",
    "        add_special_tokens = True,\n",
    "        max_length=Config.MAX_LEN,\n",
    "        pad_to_max_length=True,\n",
    "        truncation='longest_first',\n",
    "        return_tensors=\"pt\",\n",
    ")\n",
    "input_ids = encoded_text[\"input_ids\"] #[0]\n",
    "attention_mask = encoded_text[\"attention_mask\"] #[0]\n",
    "pred_model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed output\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Negative'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "output = pred_model(input_ids, attention_mask)\n",
    "pred = torch.argmax(output.data).item()\n",
    "sentiment = replace_sentiment(pred)\n",
    "sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Executing load_data\n",
      "INFO:root:Finished executing load_data\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entered no Grad\n",
      "Passed output\n",
      "Negative\n"
     ]
    }
   ],
   "source": [
    "pred_model = model.load_from_checkpoint('checkpoints/model-epoch=00-val_loss=0.00.ckpt')\n",
    "def replace_sentiment(pred):\n",
    "    if pred == 0:\n",
    "        return \"Negative\"\n",
    "    elif pred == 1:\n",
    "        return \"Positive\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "def predict(model, text):\n",
    "    encoded_text = Config.tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens = True,\n",
    "        max_length=Config.MAX_LEN,\n",
    "        pad_to_max_length=True,\n",
    "        truncation='longest_first',\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids = encoded_text[\"input_ids\"] #[0]\n",
    "    attention_mask = encoded_text[\"attention_mask\"] #[0]\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        print(\"Entered no Grad\")\n",
    "        output = model(input_ids.to('cuda'), attention_mask.to('cuda'))\n",
    "        print(\"Passed output\")\n",
    "        pred = torch.argmax(output).item()\n",
    "        sentiment = replace_sentiment(pred)\n",
    "    return sentiment\n",
    "\n",
    "y_pred = predict(pred_model, \"I hate you\")\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7da9a73fb890297e428964188f929303c9eca05bd5c6ff265c5cbfaca7107b0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
