{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports import *\n",
    "from Preprocessing_PyTorch import *\n",
    "from model import *\n",
    "from utils import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        \"\"\"\n",
    "        Initializes the classifier's parameters..\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_size = input_size #vocab_size\n",
    "        self.hidden_dim = Config.HIDDEN_SIZE\n",
    "        self.output_size = Config.OUTPUT_SIZE\n",
    "        self.LSTM_layers = Config.LSTM_LAYERS\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.embedding = nn.Embedding(self.input_size, self.hidden_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size=self.hidden_dim, hidden_size=self.hidden_dim, num_layers=self.LSTM_layers, batch_first=True)\n",
    "        self.fc1 = nn.Linear(in_features=self.hidden_dim, out_features=128)\n",
    "        self.fc2 = nn.Linear(128, self.output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \"\"\"\n",
    "        h0 = Variable(torch.zeros(self.LSTM_layers, x.size(0), self.hidden_dim, device=x.device).float())\n",
    "        c0 = Variable(torch.zeros(self.LSTM_layers, x.size(0), self.hidden_dim, device=x.device).float())\n",
    "        # h0 = torch.zeros(self.LSTM_layers, x.size(0), self.hidden_dim).float()\n",
    "        # c0 = torch.zeros(self.LSTM_layers, x.size(0), self.hidden_dim).float()\n",
    "        out = self.embedding(x)\n",
    "        out, _ = self.lstm(out, (h0,c0))\n",
    "        out = torch.relu_(self.fc1(out[:,-1,:]))\n",
    "        out = torch.sigmoid(self.fc2(out))       \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Executing:\n",
    "    \"\"\"\n",
    "    The Execution Class\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the executing class.\n",
    "        \"\"\"\n",
    "        self.batch_size = Config.BATCH_SIZE\n",
    "        self.epochs = Config.NUM_EPOCHS\n",
    "        self.lr = Config.LR\n",
    "        self.metric = Config.METRIC\n",
    "\n",
    "    def prepare_batches(self):\n",
    "        \"\"\"\n",
    "        Prepares the batches.\n",
    "        \"\"\"\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = self.df.get_data()\n",
    "        self.X_train = Variable(torch.tensor(self.X_train, dtype=torch.long))\n",
    "        self.y_train = Variable(torch.tensor(self.y_train, dtype=torch.float32))\n",
    "        self.X_test = Variable(torch.tensor(self.X_test, dtype=torch.long))\n",
    "        self.y_test = Variable(torch.tensor(self.y_test, dtype=torch.float32))\n",
    "        self.train_dataset = torch.utils.data.TensorDataset(self.X_train, self.y_train)\n",
    "        self.test_dataset = torch.utils.data.TensorDataset(self.X_test, self.y_test)\n",
    "        self.train_loader = torch.utils.data.DataLoader(self.train_dataset, batch_size=self.batch_size)\n",
    "        self.test_loader = torch.utils.data.DataLoader(self.test_dataset, shuffle=True)\n",
    "\n",
    "    def prepare_data(self):\n",
    "        \"\"\"\n",
    "        Prepares the data.\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "        self.df = Preprocessing()\n",
    "        self.df.text2seq()\n",
    "        tokenizer = self.df.get_tokenizer()\n",
    "        self.input_size = len(tokenizer.word_index) + 1\n",
    "        self.model = Classifier(self.input_size)\n",
    "        self.prepare_batches()\n",
    "        print(\"Done preparing data, done in {:.2f} seconds\".format(time.time() - start))\n",
    "    \n",
    "    def get_loaders(self):\n",
    "        return self.train_loader, self.test_loader, self.model\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    # def get_history(self):\n",
    "    #     \"\"\"\n",
    "    #     Returns the history.\n",
    "    #     \"\"\"\n",
    "    #     return self.history\n",
    "\n",
    "    # def get_model(self):\n",
    "    #     \"\"\"\n",
    "    #     Returns the model.\n",
    "    #     \"\"\"\n",
    "    #     return self.model\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     print(\"Start training\")\n",
    "#     start = time.time()\n",
    "#     execute = Executing()\n",
    "#     execute.train()\n",
    "#     end = time.time()\n",
    "#     print(f'Time taken: {end-start}')\n",
    "#     print(\"Finished training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "Done preprocessing.\n",
      "Done text2seq.\n",
      "Done preparing data, done in 307.30 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Start training\")\n",
    "execute = Executing()\n",
    "execute.prepare_data()\n",
    "train_loader, test_loader, model = execute.get_loaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_start(epoch):\n",
    "    print(f'Epoch {epoch+1}/{Config.NUM_EPOCHS}')\n",
    "\n",
    "def METRIC(outputs, labels):\n",
    "    outputs = outputs > 0.5\n",
    "    return (outputs.argmax(1) == labels).type(torch.float).sum().item()\n",
    "\n",
    "def fit(model, train_loader, test_loader, num_epochs=Config.NUM_EPOCHS):\n",
    "        \"\"\"\n",
    "        Trains the model.\n",
    "        \"\"\"\n",
    "        model.to(Config.DEVICE)\n",
    "        criterion = nn.BCELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=Config.LR)\n",
    "        trainSteps = len(train_loader.dataset) // Config.BATCH_SIZE\n",
    "        testSteps = len(test_loader.dataset) // Config.BATCH_SIZE\n",
    "        history = []\n",
    "        for epoch in range(0, num_epochs):\n",
    "            model.train()\n",
    "            start = time.time()    \n",
    "            on_epoch_start(epoch)\n",
    "            train_loss, train_acc, = 0, 0\n",
    "            val_loss, val_acc = 0, 0\n",
    "            for x, y in train_loader:\n",
    "                x = x.to(Config.DEVICE)\n",
    "                y = y.to(Config.DEVICE)\n",
    "                optimizer.zero_grad()\n",
    "                y_pred = model(x)\n",
    "                loss = criterion(y_pred, y)\n",
    "                model.train()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss\n",
    "                train_acc += METRIC(y_pred, y)\n",
    "                with torch.no_grad():\n",
    "                    #model.rnn_layer.train()\n",
    "                    for x_test, y_test in test_loader:\n",
    "                        x_test = x_test.to(Config.DEVICE)\n",
    "                        y_test = y_test.to(Config.DEVICE)\n",
    "                        model.eval()\n",
    "\n",
    "                        pred_test = model(x_test)\n",
    "                        val_acc += METRIC(pred_test, y_test)\n",
    "                        val_loss += criterion(pred_test, y_test)\n",
    "            valCorrect = val_acc / len(test_loader.dataset)\n",
    "            avgValLoss = val_loss / testSteps\n",
    "            avgTrainLoss = train_loss / trainSteps\n",
    "            trainCorrect = train_acc / len(train_loader.dataset)\n",
    "            his = {'train_loss': avgTrainLoss, 'train_accuracy': trainCorrect, 'val_loss': avgValLoss, 'val_accuracy': valCorrect}\n",
    "            history.append(his)\n",
    "            on_epoch_end(his)\n",
    "            print(f'Epoch {epoch+1} done in {time.time() - start:.2f} seconds')\n",
    "            print(\"-\"*10)\n",
    "\n",
    "def on_epoch_end(logs):\n",
    "    \"\"\"\n",
    "    Prints the logs.\n",
    "    \"\"\"\n",
    "    print(f'train_loss: {logs[\"train_loss\"]:.2f}, train_accuracy: {logs[\"train_accuracy\"]:.2f}')\n",
    "    print(f'val_loss: {logs[\"val_loss\"]:.2f}, val_accuracy: {logs[\"val_accuracy\"]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cudnn RNN backward can only be called in training mode",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m fit(model, train_loader, test_loader)\n",
      "Cell \u001b[1;32mIn[18], line 31\u001b[0m, in \u001b[0;36mfit\u001b[1;34m(model, train_loader, test_loader, num_epochs)\u001b[0m\n\u001b[0;32m     29\u001b[0m loss \u001b[39m=\u001b[39m criterion(y_pred, y)\n\u001b[0;32m     30\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m---> 31\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     32\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     33\u001b[0m train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n",
      "File \u001b[1;32mc:\\Users\\abdel\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\abdel\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cudnn RNN backward can only be called in training mode"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "fit(model, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7da9a73fb890297e428964188f929303c9eca05bd5c6ff265c5cbfaca7107b0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
