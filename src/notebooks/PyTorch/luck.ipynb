{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the program...\n",
      "The program has finished. Preprocessing took 147.78854942321777 seconds\n"
     ]
    }
   ],
   "source": [
    "from imports import *\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def load_data(path, schema, CSV=True, provide_header=False):\n",
    "    \"\"\"\n",
    "    Loads the data from the given path.\n",
    "    \"\"\"\n",
    "    if CSV:\n",
    "        if provide_header:\n",
    "            df = sc.read.format(\"csv\")\\\n",
    "                .option(\"header\", \"false\")\\\n",
    "                .schema(schema)\\\n",
    "                .load(path)\n",
    "        else:\n",
    "            df = sc.read.csv(path, header=True, inferSchema=True)\n",
    "    else:\n",
    "        df = sc.read.json(path)\n",
    "    df.persist()\n",
    "    return df\n",
    "\n",
    "def show_df(df: DataFrame)-> DataFrame:\n",
    "    \"\"\"\n",
    "    Shows the dataframe.\n",
    "    \"\"\"\n",
    "    return df.show()\n",
    "\n",
    "def describe_data(df):\n",
    "    \"\"\"\n",
    "    Prints the dataframe's statistics.\n",
    "    \"\"\"\n",
    "    return(df.describe().show())\n",
    "\n",
    "def check_nan(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Checks for NaN values in the dataframe.\n",
    "    \"\"\"\n",
    "    return(df.select([count(when(isnan(c), c)).alias(c) for c in df.columns]).show())\n",
    "\n",
    "def check_nunique(df: DataFrame)-> DataFrame:\n",
    "    \"\"\"\n",
    "    Checks the number of unique values in the dataframe.\n",
    "    \"\"\"\n",
    "    return(df.select([count(when(c.isNotNull(), c)).alias(c) for c in df.columns]).show())\n",
    "\n",
    "def drop_columns(df: DataFrame, cols: list) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Drops the columns that are not needed.\n",
    "    \"\"\"\n",
    "    return df.drop(*cols)\n",
    "\n",
    "def map_label(y):\n",
    "    \"\"\"\n",
    "    Maps the label to nicer presentation\n",
    "    \"\"\"\n",
    "    return 0 if y == 0 else 1 if y == 4 else 2\n",
    "\n",
    "def split(df: DataFrame, train_size: float, valid_size:float, test_size:float) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Splits the dataframe into train and test sets.\n",
    "    \"\"\"\n",
    "    assert (train_size + valid_size + test_size) == 1.0, \"The sum of train, valid and test sizes must be 1.0\"\n",
    "    return df.randomSplit([train_size, valid_size, test_size], seed=42)\n",
    "\n",
    "def Preprocessing(df: DataFrame, stem: bool=True) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocesses the dataframe.\n",
    "    \"\"\"\n",
    "    STOP_WORDS = stopwords.words('english')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    #text = df.select(\"text\")\n",
    "    def text_cleaning2(text: str) -> str:\n",
    "        text_cleaning_re = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "        return re.sub(text_cleaning_re, \" \", text.lower()).strip()\n",
    "\n",
    "\n",
    "    def text_cleaning(text: str) -> str:\n",
    "        \"\"\"\n",
    "        Cleans the text.\n",
    "        \"\"\"\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "        text = re.sub(r'bit.ly/\\S+', '', text)\n",
    "        text = text.strip('[link]')\n",
    "\n",
    "        # remove users\n",
    "        text = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', text)\n",
    "        text = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', text)\n",
    "\n",
    "        # remove puntuation\n",
    "        my_punctuation = '!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~•@â'\n",
    "        text = re.sub('[' + my_punctuation + ']+', ' ', text)\n",
    "\n",
    "        # remove number\n",
    "        text = re.sub('([0-9]+)', '', text)\n",
    "\n",
    "        # remove hashtag\n",
    "        text = re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)', '', text)\n",
    "        return text.lower()\n",
    "        \n",
    "    def remove_stopwords(text: str) -> str:\n",
    "        \"\"\"\n",
    "        Removes the stopwords from the text.\n",
    "        \"\"\"\n",
    "        return ' '.join([word for word in text.split() if word not in STOP_WORDS])\n",
    "\n",
    "    def stemming(text: str) -> str:\n",
    "        \"\"\"\n",
    "        Stems the text.\n",
    "        \"\"\"\n",
    "        return ' '.join([stemmer.stem(word) for word in text.split()])\n",
    "\n",
    "    def lemmatization(text: str) -> str:\n",
    "        \"\"\"\n",
    "        Lemmatization is the process of grouping together the inflected forms of a word.\n",
    "        Parameters:\n",
    "            text: str\n",
    "        \"\"\"\n",
    "        return ' '.join([lemmatizer.lemmatize(word, 'v') for word in text.split()])\n",
    "\n",
    "    def tokenize(df: DataFrame, i_p: StringType) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Tokenizes the text.\n",
    "        Parameters:\n",
    "            df: DataFrame\n",
    "        \"\"\"\n",
    "        token = Tokenizer(inputCol=i_p, outputCol=\"words\")\n",
    "        return token.transform(df)\n",
    "\n",
    "    def preprocess_text(text: str) -> str:\n",
    "        \"\"\"\n",
    "        Preprocesses the text.\n",
    "        Parameters:\n",
    "            text: the text to preprocess.\n",
    "            stem: if True, stems the text.\n",
    "        \"\"\"\n",
    "        if stem:\n",
    "            statement = stemming(remove_stopwords(text_cleaning2(text)))\n",
    "            return statement\n",
    "        else:\n",
    "            statement = lemmatization(remove_stopwords(text_cleaning2(text)))\n",
    "            return statement\n",
    "\n",
    "    cleaned_text = udf(lambda x: preprocess_text(x), StringType())\n",
    "    df = df.withColumn(\"cleaned_text\", cleaned_text(\"text\"))\n",
    "    return df\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "PATH = \"training.1600000.processed.noemoticon.csv\"\n",
    "print(\"Starting the program...\")\n",
    "beginning = time.time()\n",
    "sc = SparkSession.builder.master(\"local[*]\").appName(\"Sentiment Analysis\").getOrCreate()\n",
    "schema = StructType([\\\n",
    "    StructField(\"y\", IntegerType(), True),\\\n",
    "    StructField(\"ids\", IntegerType(), True),\\\n",
    "    StructField(\"date\", StringType(), True),\\\n",
    "    StructField(\"flag\", StringType(), True),\\\n",
    "    StructField(\"user\", StringType(), True),\\\n",
    "    StructField(\"text\", StringType(), True)])\n",
    "y_schema = StructType([\\\n",
    "    StructField(\"y\", IntegerType(), True)])\n",
    "DROPPED_COLS = ['ids', 'date', 'flag', 'user']\n",
    "df = load_data(PATH, schema, provide_header=True)\n",
    "df = drop_columns(df, DROPPED_COLS)\n",
    "df = Preprocessing(df, True)\n",
    "label = udf(lambda x: map_label(x), IntegerType())\n",
    "df = df.withColumn(\"y\", label(\"y\"))\n",
    "final_df = df.toPandas()\n",
    "final_df.to_csv(\"preprocessed.csv\", index=False)\n",
    "print(\"The program has finished. Preprocessing took {} seconds\".format(time.time() - beginning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7da9a73fb890297e428964188f929303c9eca05bd5c6ff265c5cbfaca7107b0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
